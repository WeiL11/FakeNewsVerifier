{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before vs After RL Tuning: Example Results\n",
    "\n",
    "This notebook runs the verifier **before** (default params) and **after** (RL-tuned params) on a few example claims, then plots the comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Example case\n",
    "\n",
    "We use **three claims** from the training set:\n",
    "- One likely **real** (mayor/budget) → we expect higher confidence.\n",
    "- One likely **fake** (moon landing faked) → we expect lower confidence.\n",
    "- One **multi-sentence** (mayor + crime) → we get multiple sub-claims.\n",
    "\n",
    "Each has a **ground truth** (`expected_conf`, `is_fake`) used by the RL tuner to learn better correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "if ROOT.name != \"FakeNewsVerifier\":\n",
    "    ROOT = ROOT.parent  # assume we're in notebooks/\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "import json\n",
    "from src.agents.claim_extractor import extract_claims\n",
    "from src.agents.verifier import verify_claims\n",
    "from src.agents.corrector import correct_results, DEFAULT_PARAMS\n",
    "\n",
    "# Example cases: 3 claims with ground truth\n",
    "examples = [\n",
    "    {\"claim\": \"The mayor said the budget was approved.\", \"ground_truth\": {\"is_fake\": False, \"expected_conf\": 0.75}},\n",
    "    {\"claim\": \"The moon landing was faked in Nashville.\", \"ground_truth\": {\"is_fake\": True, \"expected_conf\": 0.2}},\n",
    "    {\"claim\": \"The mayor said the budget was approved. It has been reported that crime is down.\", \"ground_truth\": {\"is_fake\": False, \"expected_conf\": 0.7}},\n",
    "]\n",
    "\n",
    "labels = [\"Mayor/budget\", \"Moon faked\", \"Mayor + crime\"]\n",
    "print(\"Example cases:\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Before results (default params)\n",
    "\n",
    "Run **verify → correct** with **no tuning**: the corrector uses hardcoded `DEFAULT_PARAMS` (e.g. `intervention_threshold=0.55`, `causal_bias=0.4`, `causal_truth=0.85`). We record the **final confidence** for each (sub-)claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_params = None  # uses DEFAULT_PARAMS\n",
    "before_confs = []  # mean final confidence per example\n",
    "before_details = []\n",
    "\n",
    "for ex, label in zip(examples, labels):\n",
    "    claims = extract_claims(ex[\"claim\"])\n",
    "    v_res, graph = verify_claims(claims)\n",
    "    corrected, _ = correct_results(v_res, graph, params=before_params)\n",
    "    confs = [r[\"confidence\"] for r in corrected]\n",
    "    mean_conf = sum(confs) / len(confs) if confs else 0.0\n",
    "    before_confs.append(mean_conf)\n",
    "    before_details.append({\"label\": label, \"confs\": confs, \"mean\": mean_conf})\n",
    "\n",
    "print(\"Before params:\", DEFAULT_PARAMS)\n",
    "print(\"Before mean confidence per example:\", before_confs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. After results (RL-tuned params)\n",
    "\n",
    "Build a small dataset from the same examples, run **tune_correction** (short PPO), then run **verify → correct** with the **learned params**. If PyTorch is missing, the tuner returns the same default params (so before ≈ after)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for ex in examples:\n",
    "    claims = extract_claims(ex[\"claim\"])\n",
    "    v_res, graph = verify_claims(claims)\n",
    "    dataset.append((v_res, graph, ex[\"ground_truth\"]))\n",
    "\n",
    "from src.rl_tuner import tune_correction\n",
    "after_params = tune_correction(dataset, num_iterations=2)\n",
    "print(\"After (tuned) params:\", after_params)\n",
    "\n",
    "after_confs = []\n",
    "for ex, label in zip(examples, labels):\n",
    "    claims = extract_claims(ex[\"claim\"])\n",
    "    v_res, graph = verify_claims(claims)\n",
    "    corrected, _ = correct_results(v_res, graph, params=after_params)\n",
    "    confs = [r[\"confidence\"] for r in corrected]\n",
    "    mean_conf = sum(confs) / len(confs) if confs else 0.0\n",
    "    after_confs.append(mean_conf)\n",
    "\n",
    "print(\"After mean confidence per example:\", after_confs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Plot: Before vs After (and expected)\n",
    "\n",
    "For each example we plot:\n",
    "- **Before**: mean final confidence with default params.\n",
    "- **After**: mean final confidence with RL-tuned params.\n",
    "- **Expected**: ground-truth `expected_conf` (what we want to match).\n",
    "\n",
    "Closer **After** to **Expected** (with few unnecessary interventions) is what the RL tuner optimizes for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "expected = [ex[\"ground_truth\"][\"expected_conf\"] for ex in examples]\n",
    "x = np.arange(len(labels))\n",
    "w = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.bar(x - w, before_confs, width=w, label=\"Before (default)\", color=\"C0\", alpha=0.8)\n",
    "ax.bar(x, after_confs, width=w, label=\"After (tuned)\", color=\"C1\", alpha=0.8)\n",
    "ax.bar(x + w, expected, width=w, label=\"Expected (ground truth)\", color=\"C2\", alpha=0.6)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_ylabel(\"Mean confidence\")\n",
    "ax.set_title(\"Before vs After RL tuning (example cases)\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "- **Example case**: three claims (mayor/budget, moon faked, mayor+crime) with known ground truth.\n",
    "- **Before**: correction uses fixed threshold 0.55 and causal bias/truth; final confidence can be off from expected.\n",
    "- **After**: correction uses learned threshold (and optionally bias) so that, on average, final confidence is closer to expected and over-correction on real claims is penalized.\n",
    "- Install PyTorch and use more iterations (`num_iterations=5–10`) for a clearer before/after difference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
